# -*- coding: utf-8 -*-
"""Backend_Openapi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vuDWPrS4_Hb0i9BzkFiWMXGZztMelia5
"""

# ================================
# Install dependencies
# ================================
!pip install openai fastapi uvicorn pyngrok pillow python-multipart nest_asyncio transformers torch torchvision

# ================================
# Imports
# ================================
import os, io, base64, random, requests, torch
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from pyngrok import ngrok
import nest_asyncio, uvicorn
from openai import OpenAI
from PIL import Image
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

# ================================
# Keys (replace these!)
# ================================
os.environ["OPENAI_API_KEY"] = ""   # <-- your OpenAI key
NGROK_KEY = ""                         # <-- your ngrok key

client = OpenAI()

# ================================
# HuggingFace fallback (free)
# ================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
hf_model_name = "nlpconnect/vit-gpt2-image-captioning"
hf_processor = ViTImageProcessor.from_pretrained(hf_model_name)
hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)
hf_model = VisionEncoderDecoderModel.from_pretrained(hf_model_name).to(device)

def generate_caption_hf(image_bytes):
    """Fallback to HuggingFace captioning"""
    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    pixel_values = hf_processor(images=image, return_tensors="pt").pixel_values.to(device)
    with torch.no_grad():
        output_ids = hf_model.generate(pixel_values, max_length=30, num_beams=5)
    return hf_tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

# ================================
# FastAPI setup
# ================================
app = FastAPI(title="Instagram Caption Generator")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ================================
# Vibe Styles
# ================================
vibe_styles = {
    "happy": ["Golden vibes â˜€ï¸", "Smiles for miles ðŸ˜Šâœ¨", "Pure sunshine ðŸŒ»"],
    "sad": ["Raindrop moods ðŸŒ§ï¸", "Quiet thoughts ðŸ’­", "Soft blues ðŸ’™"],
    "adventurous": ["Lost in the right direction ðŸ§­", "Chasing horizons ðŸŒ„", "Wander often âœˆï¸"],
    "romantic": ["Love in the air ðŸŒ¹", "Two souls, one story â¤ï¸", "Forever vibes ðŸ’•"],
    "mysterious": ["Moonlit secrets ðŸŒ™", "In the shadows ðŸ–¤", "Whispers untold ðŸ”®"],
    "energetic": ["Too much energy âš¡ðŸ”¥", "Life at full volume ðŸŽ¶", "Vibes on max ðŸš€"]
}

def enhance_caption(base_caption, vibe, user_description=""):
    vibe_data = vibe_styles.get(vibe, vibe_styles["happy"])
    extra = f" â€¢ {user_description}" if user_description else ""
    return f"{random.choice(vibe_data)} â€” {base_caption}{extra}"

# ================================
# Caption generation with fallback
# ================================
def generate_caption(image_bytes, vibe="happy", user_description=""):
    try:
        # Prepare image for OpenAI
        b64 = base64.b64encode(image_bytes).decode("utf-8")
        data_url = f"data:image/jpeg;base64,{b64}"

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an Instagram caption generator. Keep captions short, trendy, stylish, with natural emojis."},
                {"role": "user", "content": [
                    {"type": "text", "text": "Write a stylish Instagram caption for this image."},
                    {"type": "image_url", "image_url": {"url": data_url}}
                ]}
            ],
            max_tokens=60
        )

        base_caption = resp.choices[0].message.content.strip()
        return enhance_caption(base_caption, vibe, user_description), base_caption

    except Exception as e:
        # If OpenAI quota exceeded â†’ fallback to HuggingFace
        print("âš ï¸ OpenAI failed, using HuggingFace. Error:", str(e))
        base_caption = generate_caption_hf(image_bytes)
        return enhance_caption(base_caption, vibe, user_description), base_caption

# ================================
# API Models
# ================================
class CaptionURLRequest(BaseModel):
    image_url: str
    vibe: str = "happy"
    user_description: str = ""

class RefreshCaptionRequest(BaseModel):
    base_caption: str
    vibe: str = "happy"
    user_description: str = ""

# ================================
# API Endpoints
# ================================
@app.post("/generate-caption-upload")
async def generate_caption_upload(
    file: UploadFile = File(...),
    vibe: str = Form("happy"),
    user_description: str = Form("")
):
    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="File must be an image")
    contents = await file.read()
    cool_caption, base_caption = generate_caption(contents, vibe, user_description)
    return {"success": True, "caption": cool_caption, "base_caption": base_caption}

@app.post("/generate-caption-url")
async def generate_caption_url(body: CaptionURLRequest):
    response = requests.get(body.image_url, timeout=10)
    response.raise_for_status()
    cool_caption, base_caption = generate_caption(response.content, body.vibe, body.user_description)
    return {"success": True, "caption": cool_caption, "base_caption": base_caption}

@app.post("/refresh-caption")
async def refresh_caption(body: RefreshCaptionRequest):
    try:
        new_caption = enhance_caption(body.base_caption, body.vibe, body.user_description)
        return {
            "success": True,
            "caption": new_caption,
            "base_caption": body.base_caption,
            "user_description": body.user_description,
            "vibe": body.vibe
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error refreshing caption: {str(e)}")

@app.get("/vibes")
async def get_vibes():
    return {"vibes": list(vibe_styles.keys())}

# ================================
# Run with ngrok
# ================================
ngrok.set_auth_token(NGROK_KEY)
public_url = ngrok.connect(8000)
print("ðŸš€ API running at:", public_url)

nest_asyncio.apply()
uvicorn.run(app, host="0.0.0.0", port=8000)

